## План выполнения тестового задания(приоритизация задач по списку):

1. Настройка окружения: - 6ч
   - Создание boilerplate проекта для начала разработки - **Будет сгенерировано LLM, править проект все равно прийдется, но так явно быстрее**
   - Настроить graphql, mongo, docker
   - Описать все схемы и dto для данных

2. Реализовать REST API c валидацией и санитайзингом входных данных(`sanitizeObjectDeep` сгенерил через LLM, так как почитал, что in-box решений нет и нужно своей утилиткой, не хотел тратить на это время, по сути там много лишнего и получился универсальный хелпер, хватило бы только проверки на строку, но решил оставить такой. **Для себя: не забыть в одном из тестов прокинуть какие-нибудь теги в payload**) - 3ч

3. Реализация graphql(добавление резолверов) - 3ч

4. Использовать RabbitMQ - 4ч
   - Нужно будет настроить удобный клиент(коннект, добавить exchange, очередь и тд) - **возможно через LLM, но не факт, так как можно потратить больше времени на поиск проблем**
   - Реализовать подписку и паблиш сообщений

5. Написать автотесты - возможно часть тестов будет сгенерировано LLM - 5ч
**UPD.** REST тесты написаны самостоятельно, для graphql сгенерированы на основе моих тестов
6. Описать README с инструкциями по запуску и описанием проекта.
   В том числе кратко описать, как сервис может взаимодействовать с другими микросервисами и как его можно улучшить - 2ч

P.S. Весь код, который обычно генерю в LLM, проверяю в первую очередь работает он или нет. Далее уже прохожусь по коду и смотрю, что можно улучшить, что во все убрать и тд.
Так как при генерации много чего лишнего создается, boilerplate я уже почистил, добавил prettier и lint. Описал все dto, подключил mongo и тд. Что бы уже начать писать код.

PSS Структуры папок, как из ТЗ не придерживаюсь, так как там все таки для примера.
Но по факту она похоже, основное различие, это папка modules, в которой лежат конкретные модули, мне кажется так более удобно
